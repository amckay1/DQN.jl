{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CartPole-v0 example\n",
    "\n",
    "This is a detailed tutorial, so if you just want to run the example, go to the top libary directory and run `julia cartpole_mail.jl` on the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally\n",
      "I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so.5 locally\n",
      "I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally\n",
      "I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\n",
      "I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally\n",
      "I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally\n",
      "I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally\n",
      "I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally\n",
      "I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\n",
      "I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally\n"
     ]
    }
   ],
   "source": [
    "using Gym\n",
    "include(\"dqn.jl\")\n",
    "using DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the function for the default network for the CartPole-v0 example\n",
    "\n",
    "This is a default network of just one hidden layer. The user can define a more complex network architecture as long as the function still returns the required outputs and takes in the required inputs as described in the `README.md`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "createNetwork (generic function with 2 methods)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "include(\"net1.jl\") # createNetwork defined here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the OpenAI gym environment and useful variables\n",
    "\n",
    "The environment is needed for the `frame_step` function and also needs to get passed into `trainDQN` and `simulateDQN` to get the monitoring capabilities to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACTIONS = n_actions(env) = 2\n",
      "STATE_SHAPE = obs_dimensions(env) = (4,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-12-11 02:26:42,582] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "env = GymEnvironment(\"CartPole-v0\")\n",
    "@show ACTIONS = n_actions(env)         # number of valid actions\n",
    "@show STATE_SHAPE = obs_dimensions(env);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up function that interacts with the env\n",
    "\n",
    "Observe that since the state for CartPole-v0 is just a vector with four entries, there is not much of a need for preprocessing here, so the function does nothing and could be removed, but is added here for consistency. The frame step function is a wrapping of the OpenAI Gym step function and normalizes the rewards to be between $[-1,1]$. The frame step function will depend on the environment/model, but if the user is interacting with OpenAI Gym, not very much work is needed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frame_step (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(x, prev_state=nothing) = x\n",
    "\n",
    "function frame_step(action, prev_state)\n",
    "    x_t, r_t, is_terminal = step!(env, action)\n",
    "    # render(env)\n",
    "    s_t = preprocess(x_t, prev_state)\n",
    "    s_0 = is_terminal ? preprocess(reset(env), nothing) : nothing\n",
    "    s_t, r_t / 200.0, is_terminal, s_0\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing the hyperparameters\n",
    "\n",
    "Look at `dqn.jl` for descriptions of the hyperparameters included in the custom type `HyperParameters`. The fields of are the parameters that the user will need to tune to get better performance on the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN.HyperParameters(3.0f-5,0.99f0,1000,4000.0f0,0.05f0,1.0f0,20000,32,1,7500,100000,2,(4,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyper_params = HyperParameters(ACTIONS, STATE_SHAPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the DQN\n",
    "\n",
    "Running the function below will train the DQN, save the weights/vids/logs and also result in a very long output, which was why max number of episodes is set to 100. If the user wants to train the DQN, it is best to run `julia cartpole_mail.jl` on the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-12-11 02:26:49,586] Starting new video recorder writing to /home/carol/DQN.jl/test/videos/openaigym.video.0.14115.video000000.mp4\n",
      "I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 0 with properties: \n",
      "name: GeForce GTX 1080\n",
      "major: 6 minor: 1 memoryClockRate (GHz) 1.86\n",
      "pciBusID 0000:03:00.0\n",
      "Total memory: 7.92GiB\n",
      "Free memory: 7.50GiB\n",
      "I tensorflow/core/common_runtime/gpu/gpu_device.cc:972] DMA: 0 \n",
      "I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 0:   Y \n",
      "I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:03:00.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode     0. Reward=   0.070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-12-11 02:26:57,066] Starting new video recorder writing to /home/carol/DQN.jl/test/videos/openaigym.video.0.14115.video000001.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training\n",
      "Finished episode     1. Reward=   0.095\n",
      "Finished episode     2. Reward=   0.085\n",
      "Finished episode     3. Reward=   0.105\n",
      "Finished episode     4. Reward=   0.085\n",
      "Finished episode     5. Reward=   0.095\n",
      "Finished episode     6. Reward=   0.155\n",
      "Finished episode     7. Reward=   0.110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-12-11 02:27:01,235] Starting new video recorder writing to /home/carol/DQN.jl/test/videos/openaigym.video.0.14115.video000008.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode     8. Reward=   0.190\n",
      "Finished episode     9. Reward=   0.120\n",
      "Finished episode    10. Reward=   0.315\n",
      "Finished episode    11. Reward=   0.065\n",
      "Finished episode    12. Reward=   0.045\n",
      "Finished episode    13. Reward=   0.090\n",
      "Finished episode    14. Reward=   0.060\n",
      "Finished episode    15. Reward=   0.075\n",
      "Finished episode    16. Reward=   0.090\n",
      "Finished episode    17. Reward=   0.100\n",
      "Finished episode    18. Reward=   0.165\n",
      "Finished episode    19. Reward=   0.120\n",
      "Finished episode    20. Reward=   0.090\n",
      "Finished episode    21. Reward=   0.335\n",
      "Finished episode    22. Reward=   0.255\n",
      "Finished episode    23. Reward=   0.115\n",
      "Finished episode    24. Reward=   0.050\n",
      "Finished episode    25. Reward=   0.080\n",
      "Finished episode    26. Reward=   0.060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-12-11 02:27:05,319] Starting new video recorder writing to /home/carol/DQN.jl/test/videos/openaigym.video.0.14115.video000027.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode    27. Reward=   0.065\n",
      "Finished episode    28. Reward=   0.115\n",
      "Finished episode    29. Reward=   0.395\n",
      "Finished episode    30. Reward=   0.060\n",
      "Finished episode    31. Reward=   0.050\n",
      "Finished episode    32. Reward=   0.065\n",
      "Finished episode    33. Reward=   0.065\n",
      "Finished episode    34. Reward=   0.045\n",
      "Finished episode    35. Reward=   0.115\n",
      "Finished episode    36. Reward=   0.160\n",
      "Finished episode    37. Reward=   0.170\n",
      "Finished episode    38. Reward=   0.125\n",
      "Finished episode    39. Reward=   0.115\n",
      "Finished episode    40. Reward=   0.190\n",
      "Finished episode    41. Reward=   0.155\n",
      "Finished episode    42. Reward=   0.100\n",
      "Finished episode    43. Reward=   0.085\n",
      "Finished episode    44. Reward=   0.110\n",
      "Finished episode    45. Reward=   0.105\n",
      "Finished episode    46. Reward=   0.100\n",
      "Finished episode    47. Reward=   0.135\n",
      "Finished episode    48. Reward=   0.170\n",
      "Finished episode    49. Reward=   0.090\n",
      "Finished episode    50. Reward=   0.085\n",
      "Finished episode    51. Reward=   0.185\n",
      "Finished episode    52. Reward=   0.085\n",
      "Finished episode    53. Reward=   0.145\n",
      "Finished episode    54. Reward=   0.220\n",
      "Finished episode    55. Reward=   0.140\n",
      "Finished episode    56. Reward=   0.075\n",
      "Finished episode    57. Reward=   0.045\n",
      "Finished episode    58. Reward=   0.110\n",
      "Finished episode    59. Reward=   0.065\n",
      "Finished episode    60. Reward=   0.070\n",
      "Finished episode    61. Reward=   0.450\n",
      "Finished episode    62. Reward=   0.070\n",
      "Finished episode    63. Reward=   0.070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-12-11 02:27:10,993] Starting new video recorder writing to /home/carol/DQN.jl/test/videos/openaigym.video.0.14115.video000064.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode    64. Reward=   0.195\n",
      "Finished episode    65. Reward=   0.275\n",
      "Finished episode    66. Reward=   0.060\n",
      "Finished episode    67. Reward=   0.125\n",
      "Finished episode    68. Reward=   0.065\n",
      "Finished episode    69. Reward=   0.050\n",
      "Finished episode    70. Reward=   0.110\n",
      "Finished episode    71. Reward=   0.055\n",
      "Finished episode    72. Reward=   0.135\n",
      "Finished episode    73. Reward=   0.170\n",
      "Finished episode    74. Reward=   0.070\n",
      "Finished episode    75. Reward=   0.375\n",
      "Finished episode    76. Reward=   0.055\n",
      "Finished episode    77. Reward=   0.090\n",
      "Finished episode    78. Reward=   0.410\n",
      "Finished episode    79. Reward=   0.160\n",
      "Finished episode    80. Reward=   0.070\n",
      "Finished episode    81. Reward=   0.090\n",
      "Finished episode    82. Reward=   0.255\n",
      "Finished episode    83. Reward=   0.425\n",
      "Finished episode    84. Reward=   0.305\n",
      "Finished episode    85. Reward=   0.045\n",
      "Finished episode    86. Reward=   0.105\n",
      "Finished episode    87. Reward=   0.110\n",
      "Finished episode    88. Reward=   0.170\n",
      "Finished episode    89. Reward=   0.365\n",
      "Finished episode    90. Reward=   0.120\n",
      "Finished episode    91. Reward=   0.135\n",
      "Finished episode    92. Reward=   0.195\n",
      "Finished episode    93. Reward=   0.185\n",
      "Finished episode    94. Reward=   0.175\n",
      "Finished episode    95. Reward=   0.090\n",
      "Finished episode    96. Reward=   0.075\n",
      "Finished episode    97. Reward=   0.205\n",
      "Finished episode    98. Reward=   0.195\n",
      "Finished episode    99. Reward=   0.215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-12-11 02:27:18,648] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/carol/DQN.jl/test/videos')\n"
     ]
    }
   ],
   "source": [
    "hyper_params.max_num_episodes = 100\n",
    "hyper_params.observe = 25\n",
    "trainDQN(env, frame_step, createNetwork, hyper_params, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulating the DQN from saved weights\n",
    "\n",
    "Since it's obvious that training for just 100 episodes does not result in great performance, the below function loads the pre-trained weights and simulates for 2 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-12-11 02:28:45,126] Creating monitor directory /tmp/dqn/monitor/exp_CartPole-v0_2016-12-11T02:28:45.105\n",
      "[2016-12-11 02:28:45,127] Starting new video recorder writing to /tmp/dqn/monitor/exp_CartPole-v0_2016-12-11T02:28:45.105/openaigym.video.1.14115.video000000.mp4\n",
      "I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:03:00.0)\n",
      "W tensorflow/core/framework/op_kernel.cc:968] Invalid argument: You must feed a value for placeholder tensor 'placeholder_18' with dtype float\n",
      "\t [[Node: placeholder_18 = Placeholder[_class=[], dtype=DT_FLOAT, shape=[], _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n",
      "W tensorflow/core/framework/op_kernel.cc:968] Invalid argument: You must feed a value for placeholder tensor 'placeholder_18' with dtype float\n",
      "\t [[Node: placeholder_18 = Placeholder[_class=[], dtype=DT_FLOAT, shape=[], _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n",
      "W tensorflow/core/framework/op_kernel.cc:968] Invalid argument: You must feed a value for placeholder tensor 'placeholder_18' with dtype float\n",
      "\t [[Node: placeholder_18 = Placeholder[_class=[], dtype=DT_FLOAT, shape=[], _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n",
      "W tensorflow/core/framework/op_kernel.cc:968] Invalid argument: You must feed a value for placeholder tensor 'placeholder_18' with dtype float\n",
      "\t [[Node: placeholder_18 = Placeholder[_class=[], dtype=DT_FLOAT, shape=[], _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n",
      "W tensorflow/core/framework/op_kernel.cc:968] Invalid argument: You must feed a value for placeholder tensor 'placeholder_18' with dtype float\n",
      "\t [[Node: placeholder_18 = Placeholder[_class=[], dtype=DT_FLOAT, shape=[], _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n",
      "W tensorflow/core/framework/op_kernel.cc:968] Invalid argument: You must feed a value for placeholder tensor 'placeholder_18' with dtype float\n",
      "\t [[Node: placeholder_18 = Placeholder[_class=[], dtype=DT_FLOAT, shape=[], _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n",
      "W tensorflow/core/framework/op_kernel.cc:968] Invalid argument: You must feed a value for placeholder tensor 'placeholder_18' with dtype float\n",
      "\t [[Node: placeholder_18 = Placeholder[_class=[], dtype=DT_FLOAT, shape=[], _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n",
      "W tensorflow/core/framework/op_kernel.cc:968] Invalid argument: You must feed a value for placeholder tensor 'placeholder_18' with dtype float\n",
      "\t [[Node: placeholder_18 = Placeholder[_class=[], dtype=DT_FLOAT, shape=[], _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "LoadError: Tensorflow error: Status: You must feed a value for placeholder tensor 'placeholder_18' with dtype float\n\t [[Node: placeholder_18 = Placeholder[_class=[], dtype=DT_FLOAT, shape=[], _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n\t [[Node: _recv_placeholder_12_0/_10 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_77__recv_placeholder_12_0\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_placeholder_12_0)]]\n\nwhile loading In[7], in expression starting on line 1",
     "output_type": "error",
     "traceback": [
      "LoadError: Tensorflow error: Status: You must feed a value for placeholder tensor 'placeholder_18' with dtype float\n\t [[Node: placeholder_18 = Placeholder[_class=[], dtype=DT_FLOAT, shape=[], _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n\t [[Node: _recv_placeholder_12_0/_10 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_77__recv_placeholder_12_0\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_placeholder_12_0)]]\n\nwhile loading In[7], in expression starting on line 1",
      "",
      " in check_status(::TensorFlow.Status) at /home/carol/.julia/v0.5/TensorFlow/src/core.jl:101",
      " in run(::TensorFlow.Session, ::Array{TensorFlow.Port,1}, ::Array{Any,1}, ::Array{TensorFlow.Port,1}, ::Array{Ptr{Void},1}) at /home/carol/.julia/v0.5/TensorFlow/src/run.jl:96",
      " in run(::TensorFlow.Session, ::Array{Any,1}, ::Dict{Any,Any}) at /home/carol/.julia/v0.5/TensorFlow/src/run.jl:143",
      " in restore(::TensorFlow.train.Saver, ::TensorFlow.Session, ::String) at /home/carol/.julia/v0.5/TensorFlow/src/train.jl:233",
      " in simulateDQN(::Gym.GymEnvironment, ::#frame_step, ::#createNetwork, ::String, ::Int64, ::DQN.HyperParameters) at /home/carol/DQN.jl/dqn.jl:225"
     ]
    }
   ],
   "source": [
    "simulateDQN(env, frame_step, createNetwork, \"test/saved_wgts/weights-2000\", 2, hyper_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.0",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
